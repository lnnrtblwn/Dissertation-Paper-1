\section{Simulation}
In this chapter, we empirically test the performance of our proposed and the already existing estimators for \ac{SC} in different \ac{DGP}. Independent of the specific features of the data in terms of pre- and post-treatment period length and the prevailing time series structures, we proceed as follows: We simulate $T_{pre} = T_0 -1$ periods of pre-treatment and $T_{post} = T - (T_0 -1)$ periods of post-treatment data for the single treated unit and the $J$ donor units. Each estimator's main goal is to grasp the consistent patterns before treatment and accurately extend these patterns into the time after treatment. Said differently, the pre-treatment phase depicts the training set of the models and the post-treatment the validation set. To root the simulation framework as close as possible to real-world \ac{SC} application, we define $T_{pre}$ and $T_{post}$ such that their range is comparable to low-frequency macroeconomic settings, i.e. $T_{pre} \in \left\lbrace 20,50,100\right\rbrace $ and $T_{post} \in \left\lbrace 10,20,30\right\rbrace$. Furthermore, we consider two types of \ac{DGP}, a static factor process and a dynamic \ac{VAR}-process that is inspired by real \ac{GDP} processes of the G20 countries.

\subsection{Static Data Generating Processes}

In their \ac{SC} application of estimating the causal effect of California's proposition 99, \cite{abadie:2010} suppose that the (potential) outcome $Y_{i,t}^{N}$ follows a factor model of the  form 
\begin{equation*}
	Y_{i,t}^{N} = \alpha_t + \theta_t Z_i + \lambda_t \mu_i + \epsilon_{it}.
\end{equation*}
$\alpha_t$ denotes an unknown panel-invariant factor, $Z_i$ is a vector of observed panel-specific covariates, $\theta_t$ is a vector of unknown parameters, $\lambda_t$ is a vector of unknown common factors and $\mu_i$ are panel-specific unknown factor loadings. The unobserved shocks $\epsilon_{it}$ have zero mean at the panel level. 
For this specific setting, \cite{abadie:2010} show that "[...] the bias of the SC-estimator can be bounded by a function that goes to zero as the number of pre-treatment periods increases." Further the number of donor units has to be fixed.

\cite{ferman:2021} considers a de-meaned scenario without additional covariates. 
We follow Ferman and consider a similar factor \ac{DGP}. However, instead of considering a de-meaned scenario, we add time-invariant and panel-specific intercept to the (potential) outcome. Our representation of the counterfactuals therefore boils down to 
\begin{equation*}
	Y_{i,t}^{N} = \alpha_i + \lambda_t \mu_i + \epsilon_{it}.
\end{equation*}
In this simplified setting, the counterfactual is given by the composition of the unknown panel-specific factor loadings and the unknown common factors plus intercept and idiosyncratic shocks. For the sake of simplicity, Ferman considers a scenario with only two common factors, $\lambda_{1,t}$ and $\lambda_{2,t}$ and generates the data such that the (potential) outcome of the treated unit and the first half of the donor pool load exclusively with loading one on the first factor, the remaining donors load exclusively with loading one on the second factor. Therefore $\mu_i$ is a $(2 \times 1)$-rowvector with the first (second) entry being one and the second (first) entry being zero for the first (second) half of the donor pool. Further, the random variables $\alpha_i, \lambda_{1,t}, \lambda_{2,t}$ and $\epsilon_{it}$ are realizations of \ac{iid} standard normal distributions $\mathcal{N}(0,1)$. The following figure exemplifies the functionality of the \ac{DGP} with $T_{pre} = 20$ and $T_{post} = 10$.
\begin{figure}[H]
	\centering
	\includegraphics[scale=.9]{F02}
	\caption{Example Factor-\ac{DGP}}
	\label{F_02}
\end{figure}
To make the factor structure easy to see, we scaled the factor variance by $10^1$ and the error variance by $10^{-1}$. Moreover, for better eyeball inspection, we added a constant treatment effect of $\delta_{1,t} = 10$ for $t > T_0$.\footnote{In this example, the series is treated at $T_0 = 20$, while the impact of the treatment becomes noticeable one time period later, starting from $t > T_0$. Note that the actual treatment effect is irrelevant for our investigation as it is empirically observable.}  
The exhibits a clearly observable factor structure: The treatment unit and the first half of the donors (Donor 1 and 2) as well as the second half of the donors (Donor 3 and 4) share a common factor structure. Thus, the objective of each employed method is to recover the true factor structure, i. e. irrelevant of the size of donor pool to weight only the first $\frac{J}{2}$ positively.


\begin{itemize}
	%\item \cite{abadie:2010} suppose that the counterfactuals $Y_{1,t}^{N}$ for $t > T_0$ are given by a factor model of the form $Y_{i,t}^{N} = \alpha_t + \theta_t Z_i + \lambda_t \mu_i + \epsilon_{it}$. $\alpha_t$ denotes an unknown common factor with constant loadings across panel units. $Z_i$ is a vector of observed panel-specific covariates, $\theta_t$ is a vector of unknown parameters, $\lambda_t$ is a vector of unknown common facots and $\mu_i$ are panel-specific unknown factor loadings. The unobserved transitory shocks $\epsilon_{it}$ have zero mean at the panel level. For this specific setting, \cite{abadie:2010} show that "[...] the bias of the SC-estimator can be bounded by a function that goes to zero as the number of pre-treatment periods increases." Further the number of donor units has to be fixed.
	
	%\item \cite{ferman:2021} considers a de-meaned scenario without additional covariates. The representation of the counterfactuals therefore boils down to $Y_{i,t}^{N} = \lambda_t \mu_i + \epsilon_{it}$. Thus the counterfactual is given by the composition of the unknown panel-specific factor loadings and the unknown common factors plus the idiosyncratic shocks.
	
	%\item We re-estimated the Tobacco Control and the Basque application and found that the inclusion of additional covariates did not improve the predictive accuracy of the SC estimator. Therefore, we follow the simulation suggestion of \cite{ferman:2021} and root our simulation in his proposed factor model without additional covariates. 
	%\item Analogous to Ferman, we consider a setting with two common factors, $\lambda_{1,t}$ and $\lambda_{2,t}$. The potential outcomes for the treated unit and for the first half of the donor pool load exclusively with loading one on the first factor, the remaining donors load exclusively with loading one on the second factor. Therefore $\mu_i$ is a $(2 \times 1)$-rowvector with the first (second) entry being one and the second (first) entry being zero for the first (second) half of the donor pool.	
	%\item In each simulation, we simulate a total of $T_0 + T_1$ observations, where $T_0$ represents the length of the pre- and $T_1$ the length of the post-treatment period. In order to have a simulation framework that is as close as possible to real world SC-applications, we choose $T_0 \in \left\lbrace 20,50,100\right\rbrace $ and $T_1 \in \left\lbrace 10,20,30\right\rbrace $. Further, we define the size of the donor pool as $J \in \left\lbrace 5,10,15,20,25,30\right\rbrace$.
	%\item To initialize the simulation, we simulate the two common factors by drawing $T_0 + T_1$ \ac{iid}  observations from a standard normal distribution. Next, we multiply the common factors for all donors and the treatmend unit with the factor loadings $\mu_i$ and add \ac{iid} standard normal shocks. Additionally, we add panel-specific \ac{iid} standard normal intercepts such that our \ac{DGP} takes the following form: $Y_{i,t}^{N} = \gamma_i + \lambda_t \mu_i + \epsilon_{it}$. 
	%\item Figure \ref{F_02} visualizes one potential \ac{DGP} with $T_0 = 20$ and $T_1 = 10$. To make the factor structure visible, we scaled the factor variance by $10^1$ and the shock variance by $10^{-1}$. Moreover, for better eyeball inspection, we added a constant treatment effect of $\delta_{1,t} = 10$ for $t > T_0$. Note that the specific nature of the treatment effect is irrelevant for our investigation. Since $Y_{1,t}^T$ is observable for $T>T_0$, we can choose an arbitrary functional form or even implement no treatment effect at all.
	
	\item Factor-Model (FACTOR): It is assumed that the common factors and the idiosyncratic component are uncorrelated and the idiosyncratic errors are mutually uncorrelated. This suggests to estimate the common factors as linear function of the donors. A popular estimator with this property is the \ac{PC} estimator. In the training period, we obtain the predictions by regressing the treatment series on the "latent" factors. As we implemented a two-factor structure, the factors are computed by multiplying the first two eigenvalues of the covariates covariance matrix with the matrix of the covariates. The forecasts for the testing period are obtained by multiplying the factor structure of the post-treatment period with the regression coefficients of the pre-treatment regression. This model is most natural when generating data according to a factor process. Therefore, we expect it to perform best among all models.
	\item Synthetic-Control-Model (SC): In case of no additional explanatory variables, the \ac{SC}-approach reduces to a constraint regression that regresses the treatment series on the donor series given the constraint of no intercept and non-negative coefficients that sum up to one. \ac{ADH} argue that the constraints prevent the SC models from over-fitting, but they do so at the cost of a reduction in flexibility.
	\item Regularized OLS-Model (REGOLS): Our proposed model. We allow for a constant and arbitrary coefficient values. We propose a two-dimensional regularization: A ridge/l2-norm penalty that penalizes the coefficient sum and a second penalty term that shrinks the coefficient sum towards one. We identify the optimal hyperparameter-combination by applying a 50/50 train-test-split in the pre-treatment period on a two-step random grid search. In the first step, we randomly select 400 hyperparameter-combinations from a $(50 \times 50)$-grid. In the second step, we enclose the potential optimum by sequentially holding the first and the second hyperparameter fixed while increasing and decreasing the remaining hyperparameter on a coarser grid. 
	\item Elastic Net (NET): \cite{doudchenko:2016} propose to estimate the counterfactual using an elastic net regression. 
	\item OLS-Model (OLS): Finally, to verify the belief that a simple linear model overfits the training data and therefore yields poor forecasting accuracy in the testing period, we also employ an ordinary least squares model.
	%\item The described factor structure is inherent in Figure \ref{F_02}: The treatment unit and the first half of the donors (Donor\_1 and Donor\_2) as well as the second half of the donors (Donor\_3 and Donor\_4) share a common factor structure. The constant treatment effect is added after $T_0 = 20$ periods, resulting in a 10 unit vertical shift of the treatment series.
	%\item We consider five different models whose goal is to recover the true factor structure of the treatment unit in the pre-treatment period and to predict the counterfactual in the post-treatment period as accurately as possible. Put differently, we train the models on $T_0$ pre-treatment observations and test their performance using $T_1$ post-treatment observations.
	%\item The following models are employed: \textcolor{magenta}{\textbf{(Update when previous parts are written)}}
	\item All simulations are conducted with 1,000 iterations per combination of pre- and post-treatment horizon and donor quantity. \textcolor{magenta}{\textbf{(Important to distinguish different intercept cases: If the intercept of the treated series falls outside the donor-intercepts, SC exhibits a bias as it does not allow for a panel-specific constant.)}} 
	\item employ RMSE as loss function. Quadratic loss, tremendously important in practice. Reasonable approximation to realistic loss structures and mathematically convenient (\cite{diebold:2017})
\end{itemize}



\subsection{Weakly Dynamic Data Generating Processes}
\subsection{Dynamic Data Generating Processes}
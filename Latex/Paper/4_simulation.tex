\section{Simulation}
In this chapter, we empirically test the performance of our proposed and the already existing estimators for \ac{SC} in different \ac{DGP}. Independent of the specific features of the data in terms of pre- and post-treatment period length and the prevailing time series structures, we proceed as follows: We simulate $T_{pre} = T_0 -1$ periods of pre-treatment and $T_{post} = T - (T_0 -1)$ periods of post-treatment data for the single treated unit and the $J$ donor units. Each estimator's main goal is to grasp the consistent patterns before treatment and accurately extend these into the time after treatment. Said differently, the pre-treatment phase depicts the training set of the models and the post-treatment the validation set. To root the simulation framework as close as possible to real-world \ac{SC} applications, we define $T_{pre}$ and $T_{post}$ such that their range is comparable to low-frequency macroeconomic settings, i.e. $T_{pre} \in \left\lbrace 20,50,100\right\rbrace $ and $T_{post} \in \left\lbrace 10,20,30\right\rbrace$. Furthermore, we consider two types of \ac{DGP}, a static factor process and a dynamic \ac{VAR}-process that is inspired by real \ac{GDP} processes of the G20 countries.

\subsection{Static Data Generating Processes}
\subsubsection{Set up}
In their \ac{SC} application of estimating the causal effect of California's proposition 99, \cite{abadie:2010} suppose that the (potential) outcome $Y_{i,t}^{N}$ follows a factor model of the  form 
\begin{equation*}
	Y_{i,t}^{N} = \alpha_t + \theta_t Z_i + \lambda_t \mu_i + \epsilon_{it}.
\end{equation*}
$\alpha_t$ denotes an unknown panel-invariant intercept, $Z_i$ is a vector of observed panel-specific covariates, $\theta_t$ is a vector of unknown parameters, $\lambda_t$ is a vector of unknown common factors and $\mu_i$ are panel-specific unknown factor loadings. The unobserved shocks $\epsilon_{it}$ have zero mean at the panel level. 
For this specific setting, \cite{abadie:2010} show that "[...] the bias of the SC-estimator can be bounded by a function that goes to zero as the number of pre-treatment periods increases." Further the number of donor units has to be fixed. The fact, that $\alpha_t$ is panel-invariant seems minor at first glance. However, as the \ac{SC}-estimator does neither contain an intercept nor does it allow for extrapolation outside the convex hull of the donor pool, the unbiasedness of the estimator directly depends on the distribution of the intercepts. In a slightly more realistic data-generating scenario, the intercepts do not follow a degenerate point distribution with $P(X = 0) = 1$ but are drawn from a symmetric distribution centered around the origin like the standard normal. 

\cite{ferman:2021} considers a de-meaned scenario without additional covariates. Here, we follow the basic set-up of Ferman and generate data according to a similar factor model. However, we consider it more realistic to add a time-invariant and panel-specific intercept to the (potential) outcome instead of analyzing a de-meanded \ac{DGP}. Our representation of the counterfactuals therefore boils down to 
\begin{equation*}
	Y_{i,t}^{N} = \alpha_i + \lambda_t \mu_i + \epsilon_{it}.
\end{equation*}
In this simplified setting, the counterfactual is given by the composition of the unknown panel-specific factor loadings $\mu_i$ and the $F$ unknown common factors $\lambda_t = (\lambda_{1,t}, ..., \lambda_{F,t})$ plus intercept $\alpha_i$ and idiosyncratic shocks $\epsilon_{it}$. For the sake of simplicity, Ferman considers a scenario with only two common factors, $\lambda_{1,t}$ and $\lambda_{2,t}$. We proceed analogously and generate data such that the (potential) outcome of the treated unit and the first half of the donor pool load exclusively with loading one on the first factor, the remaining donors load exclusively with loading one on the second factor. Therefore $\mu_i$ is a $(2 \times 1)$-vector with the first (second) entry being one and the second (first) entry being zero for the first (second) half of the donor pool. Further, the random variables $\alpha_i, \lambda_{1,t}, \lambda_{2,t}$ and $\epsilon_{it}$ are realizations of a standard Gaussian white noise process ($\mathcal{N}(0,1)$). The following figure exemplifies the functionality of the \ac{DGP} with $T_{pre} = 20$ and $T_{post} = 10$ and a constant treatment effect of $\delta_{0,t} = 10$ for $t > T_0$.\footnote{In this example, the series $j = 0$ is treated at $T_0 = 20$, while the impact of the treatment becomes noticeable one time period later, starting from $t > T_0$. Note that the actual treatment effect is irrelevant for our investigation as it is empirically observable.}  
\begin{figure}[H]
	\centering
	\includegraphics[scale=.9]{F02}
	\caption{Example Factor-\ac{DGP}}
	\label{F_02}
\end{figure}
To make the factor structure more tangible, we scaled the factor variances by $10^1$ and the error variance by $10^{-1}$. The generated data exhibits a clearly observable factor structure: The treatment unit and the first half of the donors (Donor 1 and 2) as well as the second half of the donors (Donor 3 and 4) share a common factor. Thus, the objective of each employed method is to recover the true factor structure, i. e. irrelevant of the size of donor pool to weight only the first $\frac{J}{2}$ donors positively. Further, we see that each series possesses an own intercept, which affects especially the intercept-free models. Yet in this specific example only, the intercept variation is dominated by the factor variation due to the scaling of the variances.

\subsubsection{Employed Models}
For the static factor \ac{DGP}, we employ five models: 

SC: The first model is the ordinary \ac{SC} method without additional covariates. Therefore, this method is equivalent to a restricted \ac{OLS} regression that regresses the treatment series on the donors series given the constraint of no intercept and non-negative coefficients that sum up to one. The belief for this model is that the untuneable restrictions prevent it from overfitting the pre-treatment data but that this inelasticity comes at the costs of a reduced performance in terms of prediction and forecasting.

OLS: The second model is a usual least squares regression that regresses the treatment series on the donor series. The belief for this model is that it starts overfitting the pre-treatment data as $J$ grows large as it exhibits no regularization opportunities. Further for $J > T_0$, it is unable to provide any prediction or forecast.

NET: The third model we consider is the aforementioned elastic net as proposed by \cite{doudchenko:2016} in the context of \ac{SC}. Due to its flexible hyperparameters tuned via pre-treatment \ac{CV}, we expect this model to perform reasonable well over the entire observation window (pre- and post-treatment). For the sake of simplicity and due to the short training periods, we perform a simple 3-fold \ac{CV} and rely on the highly efficient R-implementation of the elastic net "glmnet" of \cite{friedman:2010}. In the conceptual introduction of the elastic net, we stressed the potential drawback of having no closed form solution. Consequently, we expect the model to perform slightly worse in small samples.

REGSC: The fourth model under consideration is our proposed regularized synthetic control model which can be interpreted as a mixture of the elastic net and the original \ac{SC} estimator. It is comparable to the elastic net insofar that it substitutes the Lasso-shrinkage by the inverse-Ridge-shrinkage. This substitution is motivated by the \ac{SC}-specificity of having percent-like coefficients. Thus, we expect the model to perform well especially in settings that are comparable to the original \ac{SC} setting like the static factor \ac{DGP}. Its closed form solution and the increased flexibility caused by the tuneable hyperparameters make us confident that the model performs equally well in small and in large samples as well as in the pre- and the post-treatment period. To reduce computation time, the optimal hyperparameters are obtained by 2-fold \ac{CV} in a two-step random grid-search procedure. Random hyperparameter grid search has proven to be more efficient than manual grid search both theoretically and empirically. See for instance \cite{bergstra:2012} for a careful discussion of hyperparameter optimization. Specifically, we start by spaning a large $50 \times 50$ grid with $\lambda_1$ ranging from 5 to 3,125 and $\lambda_2$ ranging from 10 to $10^7$ and randomly select 400 out of the 2,500 $\lambda_1$-$\lambda_2$-combinations.\footnote{In sensitivity checks, we found that wider intervals did not change the location of the potential optimum. However, the optimal range will always be case-specific.} In the first step of the procedure, we identify the optimal hyperparameter combination of the initialized grid by applying 2-fold \ac{CV}. Based on the result of the first step, we enclose the potential optimum in the second step by sequentially holding the first and the second hyperparameter fixed while increasing and decreasing the remaining hyperparameter on a coarser grid. Note that a more efficient \ac{CV}-procedure could potentially improve the \ac{REGSC}-method.

FACTOR: The data of this process is generated such that the common factors and the idiosyncratic component are uncorrelated and that the idiosyncratic errors are mutually uncorrelated. Thus, the static factor model which estimates the common factors as linear function of the donors is the most natural candidate model. The \ac{PC} estimator is a popular estimator for the factor model. It is employed as follows: In the pre-treatment period, we obtain the predictions by regressing the treatment series on the "latent" factors. As we implemented a two-factor structure, the factors are computed by multiplying the first two eigenvalues of the covariates matrix of the donors with the matrix of the covariates of the donors. The forecasts for the post-treatment period are obtained by multiplying the factor structure of the post-treatment period with the regression coefficients of the pre-treatment regression. As this model directly build upon the \ac{DGP}, it is our benchmark-model and we expect it to perform best among all candidates.\textcolor{magenta}{\textbf{(@ Jörg Breitung: why no overfitting?)}} 

For each of the 9 combinations of pre- and post-treatment period length and the 6 investigated donor group sizes, we simulated 1,000 static factor processes as described above.\footnote{Remember that $T_{pre} \in \left\lbrace 20,50,100\right\rbrace $, $T_{post} \in \left\lbrace 10,20,30\right\rbrace$ and $J \in \left\lbrace 5,10,15,20,25,30\right\rbrace$.} This extensive simulation provides us with a total of 54,000 processes that are analyzed with respect to the following location and dispersion metrics in the post-treatment period:
\ac{RMSFE}: The \ac{RMSFE} is the central loss function of our analysis. It has the following form:
$$RMSFE(m) = \left(\frac{1}{T - T_0} \sum_{t = T_0}^{T} \left( y_{0,t} - \delta_{0,t} - \widehat{y}_{0,t}(m)\right) ^2 \right)^{1/2},$$
where $m$ present one of the five employed models. Due to its quadratic nature, it is not only a reasonable approximation to realistic loss structures but also mathematically convenient \cite{diebold:2017}. Note that the treatment effect $\delta_{0,t}$ is subtracted from $y_{0,t}$ to re-calibrate the process towards its origin.  
	
Bias: The \ac{RMSFE} is unable to distinguish between over- and underestimation as deviations from the true target quantity are squared. The bias is directly related to the $RMSE$ but provides a more detailed measure in terms of error location. It is computed as follows:
$$BIAS_m = \frac{1}{T - T_0} \sum_{t = T_0}^{T} \widehat{y}_m - (y_{0,t} - \delta_{0,t}),$$
such that negative values indicate under- and positive values overestimation. This precision metric is especially important when analyzing intercept-free models as these models will exhibits a bias whenever the treatment intercept falls outside the donor intercepts. 

\ac{MZ} regression: The \ac{MZ} regression tests the forecast optimality in a different and more holistic way by regressing the true value on its predicted value in the post-treatment period: 
$$y_{0,t} = \beta_0 + \beta_1 \widehat{y}_{0,t} \text{ for } t \geq T- T_0.$$
If the forecast is optimal, we expect to observe that $(\beta_0, \beta_1) = (0,1)$, an hypothesis that is directly testable by a simple F-test. Similar to the \ac{RMSFE} we can report average, simulation-overarching quantities without wiping out crucial details like negative and positive biases. In our simulation, we report the share of iterations in which the F-test accepted the joint hypothesis of $(\beta_0, \beta_1) = (0,1)$ at the conventional significance level of 5\%. The closer this share to unity, the closer the specific forecast to the optimal quantity. Due to varying sample sizes, however, those shares are only interpretable at the within-simulation level.
	
Variance: To observe the variability of the employed models, we also compute their intra-simulation variances as
$$VAR(m) = \left(\frac{1}{T - T_0} \sum_{t = T_0}^{T} \widehat{y}_{0,t}(m) - \overline{\widehat{y}}_{0,t}(m) \right) ^2. $$


\subsubsection{Results}
The full simulation results can be found in table format in the appendix \ref{Static simulation} where we group the tables at the level of the six analyzed donor quantities. Here, we only present the central tendencies of the simulation. To do so, consider the following figure that plots the average \ac{RMSFE} of the models against the size of the donor pool for $T_{pre} = 50$ and $T_{post} \in \left\lbrace 10,20,30\right\rbrace$.\footnote{As we found that the length of the post-treatment period is less important to explain the models performance, we pool the three elements of $T_{post}$ at this point. The depicted means are thus computed on the basis of 3,000 iterations each. The same figures can be found in appendix \ref{Static simulation} for $T_{pre} = 20$ and $T_{pre} = 100$.} 

%\textcolor{magenta}{\textbf{(Important to distinguish different intercept cases: If the intercept of the treated series falls outside the donor-intercepts, SC exhibits a bias as it does not allow for a panel-specific constant.)}} 


\begin{figure}[H]
	\centering
	\includegraphics[scale=.9]{F07}
	\caption{Simulation Performance for $T_{pre} = 50$ and $T_{post} \in \left\lbrace 10,20,30\right\rbrace$}
	\label{F_07}
\end{figure}

Five observations stand out: First, as expected, the unrestricted \ac{OLS} regression starts to overfit the pre-treatment data quite fast indicated by a \ac{RMSFE} that starts to increase from $J = 10$ onward. This tendency consistently aggravates as $J$ approaches $T_{pre}$. Second, the remaining models seem to successfully distinguish between systematic pre-treatment patterns and noise as their forecasts improve with increasing $J$. Third, in terms of relative performance, for obvious reasons, the factor models extrapolates the process best into the post-treatment period. Fourth, the elastic net and the proposed \ac{REGSC} estimator perform comparable with slight comparative advantages toward the latter. These differences are more pronounced as $J$ increases. Last but not least, the \ac{SC} estimator, though being able to not overfit the training data, consistently performs worse than the elastic net and the \ac{REGSC} estimator. 

It has been stressed that the \ac{RMSFE} should not be the only precision metric when evaluating  forecast performances as it is unable to detect over- and underestimation. On the other hand, the bias, though being able to flag iteration-specific over- and underestimations, can indicate spurious optimality when it is aggregated into a single metric like the mean.\footnote{Consider for instance a model that forecasts $\left\lbrace -1,1\right\rbrace $, each in 50\% of the cases for a quantity whose optimal forecast is 0 in 100\% of the cases. This model is far from optimal but the mean bias is 0.} To solve this problem, we first focus on the bias-distributions: Above, we argued, that intercept-free models like the \ac{SC}-method will exhibit a positive (negative) bias whenever the mean of the treatment-series exceeds (falls below) the means of the donor series. In our simulation, the intercepts of the series are \ac{iid} realizations of standard normal distributions. Therefore, the probability that the mean of the treatment series falls below (exceeds) all donor means equals $\frac{1}{J+1}$.\footnote{For each donor quantity $J$, there are $(J+1)!$ total orderings. In $\frac{(J+1)!}{J+1}$ of the cases, the intercept of the treatment series is the most extreme. This translate to a probability of $\frac{1}{J+1}$. } In the following figure, we extracted the iterations for which the treatment intercept was the most extreme (for all donor group sizes $J$), grouped these observations according to minimum (positive bias)/ maximum (negative bias) and plotted the post-treatment bias density for the \ac{SC} and the \ac{REGSC} model. 
\begin{figure}[H]
	\centering
	\includegraphics[scale=.9]{F11}
	\caption{Bias-distribution for the SC and the REGSC model}
	\label{F_09}
\end{figure}

The aforementioned bias-problem becomes immediately apparent: Though both estimators have an average bias that is close to 0, the \ac{SC} method exhibits severe positive/ negative biases if the treatment intercept falls outside the convex hull of the donors intercepts. In contrast, the \ac{REGSC} model as well as the remaining models that include an intercepts do not face this issue. In appendix \ref{Static simulation}, we group the observations the model and the $T_{pre}$ level and plot the corresponding densities. On a summarized level, we see, as expected, that the bias-distributions of all models become more tightly centered around zero the longer the per-treatment periods and that the factor model, which serves as a benchmark model, outperforms all other models. Among these models the elastic net and the \ac{REGSC} model are the closest contenders. This pattern also becomes visible when we consider the \ac{MZ}-acceptance rate as in the following table:\footnote{As the benchmark factor model performs best in all cases, we report the second best model in column 8.}

\begin{table}[H]
	\centering
	\noindent
	\caption{MZ-Acceptance rates for $T_{pre} \in \left\lbrace 20,50,100\right\rbrace$ and $J \in \left\lbrace 5,10,15,20,25,30\right\rbrace$}
	\scalebox{1}{
		\begin{tabular}{c c | C{1.5cm} | C{1.5cm} | C{1.5cm} | C{1.5cm} | C{1.5cm} | C{1.5cm} }
			\toprule[1.5pt]
			\multicolumn{1}{c}{$\boldsymbol{T_{pre}}$} & \multicolumn{1}{c}{\textbf{Donors}}& \multicolumn{1}{c}{\textbf{SC}} & \multicolumn{1}{c}{\textbf{OLS}} & \multicolumn{1}{c}{\textbf{REGSC}}& \multicolumn{1}{c}{\textbf{NET}}& \multicolumn{1}{c}{\textbf{FACTOR}} & \multicolumn{1}{c}{\textbf{2nd best}}\\
			\cmidrule(lr){1-8}
			20&5&0.5650&0.6617&0.7413&0.7648&0.8143&NET\\
			20&10&0.6897&0.3777&0.7577&0.7479&0.8050&REGSC\\
			20&15&0.7343&0.1153&0.7517&0.7486&0.8190&REGSC\\
			20&20&0.7600&NA&0.7380&0.7193&0.7993&SC\\
			20&25&NA&NA&0.7427&0.7420&0.7987&REGSC\\
			20&30&NA&NA&0.7433&0.7475&0.8227&NET\\
			\cmidrule(lr){1-8}
			50&5&0.5857&0.8707&0.8677&0.8820&0.8947&NET\\
			50&10&0.7503&0.8237&0.8787&0.8783&0.8940&REGSC\\
			50&15&0.8067&0.7123&0.8703&0.8680&0.8923&REGSC\\
			50&20&0.8103&0.6007&0.8757&0.8717&0.8910&REGSC\\
			50&25&0.8423&0.4333&0.8803&0.8750&0.8920&REGSC\\
			50&30&0.8510&0.3023&0.8680&0.8667&0.8923&REGSC\\
			\cmidrule(lr){1-8}
			100&5&0.6200&0.9100&0.9057&0.9150&0.9193&NET\\
			100&10&0.7757&0.9120&0.9207&0.9277&0.9277&NET\\
			100&15&0.8030&0.8743&0.9140&0.9067&0.9183&REGSC\\
			100&20&0.8430&0.8480&0.9180&0.9157&0.9260&REGSC\\
			100&25&0.8540&0.8097&0.9167&0.9117&0.9247&REGSC\\
			100&30&0.8793&0.7537&0.9177&0.9127&0.9257&REGSC\\
			\bottomrule[1.5pt]
	\end{tabular}}
	\label{table:table S.7}
\end{table}

For circumstances of few donors or cases where $J \gg T_{pre}$, the Lasso-penalty seems to be superior to the inverse-Ridge-penalty as in those cases, the elastic net outperforms the \ac{REGSC} at a small margin. Further, we see that the \ac{SC} regularization demonstrates its strength particularly in settings of many donors and few training observations but exhibits weaknesses in cases of few donors. For example, for $T_{pre} = 20$, the model improves its \ac{MZ}-rate from $0.5650$ to $0.7600$ by about $135\%$. No other models features such high absolute and relative improvements. If $\frac{T_{pre}}{J} < 1$, the simple \ac{SC} approach without covariates struggles with parameter identification. Consequently, it's not possible to compute \ac{MZ}-rates for rows $5$ and $6$. Note, however, that the original \ac{SC}-method does not face such limitations and is able to compute the synthetic control even when $J \gg T_{pre}$. 

The static simulation study can be concluded as follows: With exception of the OLS model, all employed models are capable to distinguish systematic pre-treatment patterns from noise and do not overfit the training data. The simplified version of the \ac{SC} method is particularly promising, when $\frac{T_{pre}}{J} \approx 1$, however, as stressed by \cite{abadie:2010} it is of crucial importance to verify that the treatment intercept lies inside the bandwidth of the donor intercepts as substantial biases results if ignored. The elastic net and \ac{REGSC} model include an intercept and allow more flexible weight coefficients and can thus be applied independently of the mean characteristics of the series. In particular, the former promises high forecast accuracy when the donor pool is small. In the remaining 12 out of 18 considered cases, we find that our proposed \ac{REGSC} estimator performs best.\footnote{A similar conclusion can also be drawn on the basis of the \ac{RMSFE} instead of the \ac{MZ}-acceptance rates. The corresponding is postponed to appendix \ref{Static simulation}}
Before we test our model outside the world of synthesized data, we show the simulation results of a dynamic \ac{DGP} that is inspired by the real world covariance structure of the \ac{GDP}s of the G20 countries.


\subsection{Dynamic Data Generating Processes}
\section{Simulation}
In this chapter, we empirically test the performance of our proposed and the already existing estimators for \ac{SC} in different \ac{DGP}. Independent of the specific features of the data in terms of pre- and post-treatment period length and the prevailing time series structures, we proceed as follows: We simulate $T_{pre} = T_0 -1$ periods of pre-treatment and $T_{post} = T - (T_0 -1)$ periods of post-treatment data for the single treated unit and the $J$ donor units. Each estimator's main goal is to grasp the consistent patterns before treatment and accurately extend these into the time after treatment. Said differently, the pre-treatment phase depicts the training set of the models and the post-treatment the validation set. To root the simulation framework as close as possible to real-world \ac{SC} applications, we define $T_{pre}$ and $T_{post}$ such that their range is comparable to low-frequency macroeconomic settings, i.e. $T_{pre} \in \left\lbrace 20,50,100\right\rbrace $ and $T_{post} \in \left\lbrace 10,20,30\right\rbrace$. Furthermore, we consider two types of \ac{DGP}, a static factor process and a dynamic \ac{VAR}-process that is inspired by real \ac{GDP} processes of the G20 countries.

\subsection{Static Data Generating Processes}

In their \ac{SC} application of estimating the causal effect of California's proposition 99, \cite{abadie:2010} suppose that the (potential) outcome $Y_{i,t}^{N}$ follows a factor model of the  form 
\begin{equation*}
	Y_{i,t}^{N} = \alpha_t + \theta_t Z_i + \lambda_t \mu_i + \epsilon_{it}.
\end{equation*}
$\alpha_t$ denotes an unknown panel-invariant factor, $Z_i$ is a vector of observed panel-specific covariates, $\theta_t$ is a vector of unknown parameters, $\lambda_t$ is a vector of unknown common factors and $\mu_i$ are panel-specific unknown factor loadings. The unobserved shocks $\epsilon_{it}$ have zero mean at the panel level. 
For this specific setting, \cite{abadie:2010} show that "[...] the bias of the SC-estimator can be bounded by a function that goes to zero as the number of pre-treatment periods increases." Further the number of donor units has to be fixed. The fact, that $\alpha_t$ is panel-invariant seems minor at first glance. However, as the \ac{SC}-estimator does neither contain an intercept nor does it allow for extrapolation outside the convex hull of the donor pool, the unbiasedness of the estimator directly depends on the distribution of the intercepts. In a slightly more realistic data-generating scenario, the intercepts do not follow a degenerate point distribution with $P(X = 0) = 1$ but are drawn from a symmetric distribution centered around the origin like the standard normal. 

\cite{ferman:2021} considers a de-meaned scenario without additional covariates. In our first simulation, we follow the basic set-up of Ferman and generate data according to a similar factor model. However, we consider it more realistic to add a time-invariant and panel-specific intercept to the (potential) outcome instead of analyzing a de-meanded \ac{DGP}. Our representation of the counterfactuals therefore boils down to 
\begin{equation*}
	Y_{i,t}^{N} = \alpha_i + \lambda_t \mu_i + \epsilon_{it}.
\end{equation*}
In this simplified setting, the counterfactual is given by the composition of the unknown panel-specific factor loadings $\mu_i$ and the $F$ unknown common factors $\lambda_t = (\lambda_{1,t}, ..., \lambda_{F,t})$ plus intercept $\alpha_i$ and idiosyncratic shocks $\epsilon_{it}$. For the sake of simplicity, Ferman considers a scenario with only two common factors, $\lambda_{1,t}$ and $\lambda_{2,t}$. We proceed analogously and generate data such that the (potential) outcome of the treated unit and the first half of the donor pool load exclusively with loading one on the first factor, the remaining donors load exclusively with loading one on the second factor. Therefore $\mu_i$ is a $(2 \times 1)$-column-vector with the first (second) entry being one and the second (first) entry being zero for the first (second) half of the donor pool. Further, the random variables $\alpha_i, \lambda_{1,t}, \lambda_{2,t}$ and $\epsilon_{it}$ are realizations of \ac{iid} standard normal distributions $\mathcal{N}(0,1)$. The following figure exemplifies the functionality of the \ac{DGP} with $T_{pre} = 20$ and $T_{post} = 10$ and a constant treatment effect of $\delta_{0,t} = 10$ for $t > T_0$.\footnote{In this example, the series $j = 0$ is treated at $T_0 = 20$, while the impact of the treatment becomes noticeable one time period later, starting from $t > T_0$. Note that the actual treatment effect is irrelevant for our investigation as it is empirically observable.}  
\begin{figure}[H]
	\centering
	\includegraphics[scale=.9]{F02}
	\caption{Example Factor-\ac{DGP}}
	\label{F_02}
\end{figure}
To make the factor structure more tangible, we scaled the factor variances by $10^1$ and the error variance by $10^{-1}$. The generated data exhibits a clearly observable factor structure: The treatment unit and the first half of the donors (Donor 1 and 2) as well as the second half of the donors (Donor 3 and 4) share a common factor. Thus, the objective of each employed method is to recover the true factor structure, i. e. irrelevant of the size of donor pool to weight only the first $\frac{J}{2}$ donors positively. Further, we see that each series possesses an own intercept. Yet in this specific example only, the intercept variation is dominated by the factor variation due to the scaling of the variances.

\textit{EMPLOYED MODELS} \\
For the static factor \ac{DGP}, we employ five models: 

SC: The first model is the ordinary \ac{SC} method without additional covariates. Therefore, this method is equivalent to a restricted \ac{OLS} regression that regresses the treatment series on the donors series given the constraint of no intercept and non-negative coefficients that sum up to one. The belief for this model is that the untuneable restrictions prevent it from overfitting the pre-treatment data but that this inelasticity comes at the costs of a reduced predictive performance.

OLS: The second model is a usual least squares regression that regresses the treatment series on the donor series. The belief for this model is that it starts overfitting the pre-treatment data as $J$ grows large as it exhibits no regularization opportunities. Further for $J > T_0$, it is unable to provide any prediction or forecast.

NET: The third model we consider is the aforementioned elastic net as proposed by \cite{doudchenko:2016} in the context of \ac{SC}. Due to its flexible hyperparameters tuned via pre-treatment \ac{CV}, we expect this model to perform reasonable well over the entire observation window (pre- and post-treatment). For the sake of simplicity and due to the short training periods, we perform a simple 3-fold \ac{CV} and rely on highly efficient R-implementation of the elastic net "glmnet" of \cite{friedman:2010}. In the conceptual introduction of the elastic net, we stressed the potential drawback of having no closed form solution. As a consequence, we assume the model to perform worse in small samples.

REGSC: The fourth model under consideration is our proposed regularized synthetic control model which can be interpreted as a mixture of the elastic net and the original \ac{SC} estimator. It is comparable to the elastic net insofar that it simply substitutes the Lasso-shrinkage by the inverse-Ridge-shrinkage. This substitution is motivated by the \ac{SC}-specificity of having percent-like coefficients. Thus, we expect the model to perform well especially in settings that are comparable to the original \ac{SC} setting like the static factor \ac{DGP}. Its closed form solution and the increased flexibility caused by the tuneable hyperparameters make us confident that the model performs equally well in small and in large samples as well as in the pre- and the post-treatment period. To reduce computation time, the optimal hyperparameters are obtained by 2-fold \ac{CV} in a two-step random grid-search procedure. Random hyperparameter grid search has proven to be more efficient than manual grid search both theoretically and empirically. See for instance \cite{bergstra:2012} for a careful discussion of hyperparameter optimization. Specifically, we start by spaning a large $50 \times 50$ grid with $\lambda_1$ ranging from 5 to 3,125 and $\lambda_2$ ranging from 10 to $10^7$ and randomly select 400 out of the 2,500 $\lambda_1$-$\lambda_2$-combinations.\footnote{In sensitivity checks, we found that wider intervals did not change the location of the potential optimum. However, the optimal range will always be case-specific.} In the first step of the procedure, we identify the optimal hyperparameter combination of the initialized grid by applying 2-fold \ac{CV}. Based on the result of the first step, we enclose the potential optimum in the second step by sequentially holding the first and the second hyperparameter fixed while increasing and decreasing the remaining hyperparameter on a coarser grid. Note that a more efficient \ac{CV}-procedure could potentially improve the \ac{REGSC}-method.

FACTOR: The data of this process is generated such that the common factors and the idiosyncratic component are uncorrelated and that the idiosyncratic errors are mutually uncorrelated. Thus, the static factor model which estimates the common factors as linear function of the donors is the most natural candidate model. The \ac{PC} estimator is a popular estimator for the factor model. It is employed as follows: In the pre-treatment period, we obtain the predictions by regressing the treatment series on the "latent" factors. As we implemented a two-factor structure, the factors are computed by multiplying the first two eigenvalues of the covariates matrix of the donors with the matrix of the covariates of the donors. The forecasts for the post-treatment period are obtained by multiplying the factor structure of the post-treatment period with the regression coefficients of the pre-treatment regression. As this model directly build upon the \ac{DGP}, it is our benchmark-model and we expect it to perform best among all candidates.\textcolor{magenta}{\textbf{(@ JÃ¶rg Breitung: why no overfitting?)}} 

For each of the 9 combinations of pre- and post-treatment period length and the 6 investigated donor group sizes, we simulated 1,000 processes.\footnote{Remember that $T_{pre} \in \left\lbrace 20,50,100\right\rbrace $, $T_{post} \in \left\lbrace 10,20,30\right\rbrace$ and $J \in \left\lbrace 5,10,15,20,25,30\right\rbrace$.} This extensive simulation provides us with a total of 54,000 processes that are analyzed with respect to the following precision and dispersion metrics in the post-treatment period:
\begin{itemize}
	\item RMSFE: The RMSE is the central loss function of our analysis. It has the following form:
	$$RMSFE(m) = \left(\frac{1}{T - T_0} \sum_{t = T_0}^{T} \left( y_{0,t} - \delta_{0,t} - \widehat{y}_{0,t}(m)\right) ^2 \right)^{1/2},$$
	
	where $m$ present one of the five employed models. Due to its quadratic nature, it is not only a reasonable approximation to realistic loss structures but also mathematically convenient \cite{diebold:2017}. 
	\item Bias: The RMSE is unable to distinguish between over- and underestimation as deviations from the true target quantity are squared. The bias is directly related to the $RMSE$ but provides a more detailed measure in terms of error location. It is computed as follows:
	$$BIAS_m = \frac{1}{T - T_0} \sum_{t = T_0}^{T} \widehat{y}_m - (y_{0,t} - \delta_{0,t}),$$
	such that negative values indicate under- and positive values overestimation. This precision metric is especially important when analyzing intercept-free models as these models will exhibits a bias whenever the treatment intercept falls outside the donor intercepts. 
	\item \ac{MZ} regression: The \ac{MZ} regression tests the forecast optimality in a different and more holistic way by regressing true value on its predicted value in the post-treatment period: 
	$$y_{0,t} = \beta_0 + \beta_1 \widehat{y}_{0,t} \text{ for } t \geq T- T_0.$$
	If the forecast is optimal, we expect to observe that $(\beta_0, \beta_1) = (0,1)$, an hypothesis is directly testable by a simple F-Test. In comparison to the RMSE and to the bias, this approach superior because we can report averaged quantities (e.g. the simulation share for which the \ac{MZ} F-test rejects the joint hypothesis at the 5\% significance level) without averaging important differences away (bias).  
	
	\item Variance: 

	
\end{itemize}

\begin{itemize}
	\item \textcolor{magenta}{\textbf{(Important to distinguish different intercept cases: If the intercept of the treated series falls outside the donor-intercepts, SC exhibits a bias as it does not allow for a panel-specific constant.)}} 
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[scale=.9]{F07}
	\caption{Simulation Performance for $T_{pre} = 50$ and $T_{post} \in \left\lbrace 10,20,30\right\rbrace$}
	\label{F_07}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=.9]{F09}
	\caption{\ac{MZ} p-values vs. respective bias for the REGSC-model}
	\label{F_08}
\end{figure}

\subsection{Dynamic Data Generating Processes}